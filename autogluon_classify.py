import pandas as pd
import os
from autogluon.tabular import TabularPredictor as task
import loaders_multilabel as mll
from autogluon.core.utils import infer_problem_type
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import roc_curve
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import time
## ------------------------------ Training ------------------------------#
# Trains multiple models and implements stacking and ensembling. 
# - Size of validation data determines VARIANCE of performance estimates
# NOTE: You can specify proportion of validation set manually: fit(): tuning_data=validation_data
# Autogluon takes dataframe 'dataf' training features with target column; target: name of target column
def train1(dataf, targetcol, ptype, malinst, hostfts, topk):
	dir = os.getcwd()+'/agModels/'+str(malinst)+"_top"+str(topk)+"_"+str(hostfts)+"/"
	if not os.path.exists(dir):
		os.system("mkdir "+dir)

	if ptype == "multiclass":
		predictor = task(label=targetcol, path=dir, eval_metric='balanced_accuracy').fit(dataf, time_limit=300, verbosity=4, presets='best_quality')
	else:
		print("BINARY PROBLEM")
		#eval_metric='balanced_accuracy'
		predictor = task(label=targetcol, path=dir, eval_metric='balanced_accuracy').fit(dataf, verbosity=4)
	##print(predictor.fit_summary(show_plot=True))   #(predictor.get_info())
	return predictor


# Multi layer stacking takes predictions of base models and feeds to stack models
# AG will auto choose k: 10 fold bagging, n: 20 bagging repeats,
# L: 2 layers of models in stack followed by weighted-ensemble (weight model's decision;high weight to model that performed well);
# aggregate model predictions based on model weights and produce final prediction.
def train_multilayerstacking(traindf, target, ptype, malinst, hostfts, topk):
	dir = os.getcwd()+'/agModels/stacked/'+str(malinst)+"_top"+str(topk)+"_"+str(hostfts)+"/"
	if not os.path.exists(dir):
		os.system("mkdir "+dir)

	if ptype == "multiclass":
		predstack = task(label=target, path=dir).fit(train_data= traindf, time_limit=300, auto_stack=True, verbosity=3)
	else:
		print("BINARY PROBLEM")
		predstack = task(label=target, path=dir, eval_metric='balanced_accuracy').fit(train_data= traindf, auto_stack=True, verbosity=3)
	#predictions = predictor.predict()
	#print(predstack.get_info())
	##print(":::::::Train Summary:::::\n", predstack.fit_summary(show_plot=True))
	return predstack


# ----------------------------Evaluation--------------------------------#
def graph_prec_recall(ytest, ypred, fname):
	precision, recall, thresh = precision_recall_curve(ytest, ypred)
	print(precision, recall, thresh)
	fig, ax = plt.subplots()
	ax.plot(recall, precision, marker='.', color='blue')
	#add axis labels to plot
	ax.set_title('Precision-Recall')
	ax.set_ylabel('Precision')
	ax.set_xlabel('Recall')
	#display plot
	plt.show()
	fig.savefig("output/PREC-REC_"+fname+".png")
	return

def getroc_curve(ytest, ypred_proba, fname):
	# Compute ROC curve and ROC area for each class
	print(ytest, ypred_proba)
	# Roc for positive class classifications
	fpr, tpr, _ = roc_curve(ytest, ypred_proba.iloc[:, 1])
	print("ROC: FPR, TPR", fpr, tpr)
	fig, ax = plt.subplots()
	ax.plot([0, 1], [0, 1], linestyle='--')
	ax.plot(fpr, tpr, marker='.')
	ax.set_title('ROC Curve')
	ax.set_xlim([0.0, 1.0])
	ax.set_ylim([0.0, 1.05])
	ax.set_ylabel('True Positive Rate')
	ax.set_xlabel('False Positive Rate')
	#display plot
	plt.show()
	fig.savefig("output/ROC_"+fname+".png")
	return

def study_misclassified(ytrue, ypred, ypredproba):
	#print("Types: ", type(ytrue), type(ypred), type(ypredproba))
	for i in range(0, ytrue.size):
		#if i in yt and i in yp and i in ypredproba:
		yt = ytrue[i]
		yp = ypred[i]
			#print(yt, yp)
		yppben = ypredproba.iloc[i]
		if not yt == yp:
			print("YTrue: %d, Ypred: %d"%(yt, yp))
			print(yppben)
	return

def rank_positives(ytrue, ypred, ypredproba):
	ranked = []
	for i in range(0, ytrue.size):
		#if i in yt and i in yp and i in ypredproba:
		yt = ytrue[i]
		yp = ypred[i]
			#print(yt, yp)
		yppben = ypredproba.iloc[i][1]
		if yp == 1:
			ranked += [(yppben, yt, yp, yt==yp)]

	ranked.sort(reverse=True)
	print("Positive classifications:\n Confidence, True Label, Predicted, True Positive?")
	for x in ranked:
		print(x)
	return ranked

def confusionmatrix(ytest, ypred):
	tn, fp, fn, tp = confusion_matrix(ytest, ypred).ravel()
	fpr = fp/(fp+tn)
	fpp = 0
	for tr, pred in zip(ytest, ypred):
		if tr == 0 and pred == 1:
			##print(tr, pred)
			fpp += 1

	print("tn:%d fp:%d fn:%d tp:%d"%(tn, fp, fn, tp))
	#print("Fpp: ", fpp)
	print("FPR: ", fpr*100)
	return [tn,fp,fn,tp]

# ------------------------- Test functions ------------------------------#
# Prediction is done using model with best validation performance. AG creates
# ensembles to create maximized validation performance.
def test1(Xtest, ytest, pred, testdf, traindf, calcftimpo=True):
	ypred = pred.predict(Xtest)
	ypredproba = pred.predict_proba(Xtest)
	perf = pred.evaluate_predictions(y_true=ytest, y_pred=ypred, auxiliary_metrics= True)
	##perf = pred.evaluate_predictions(y_true=ytest, y_pred=ypredproba, auxiliary_metrics= True)
	#print("[*]Predictions: ", ypred)
	print("[*]Confidence in predictions:\n")
	print(pd.DataFrame(ypredproba, columns=pred.class_labels))
	# Each model score
	modelperf = pred.leaderboard(testdf, silent= True)
	print("[*]Model performance breakdown on Test data:")
	print(modelperf)
	print("Performance evaluation on test data: ", perf)
	# PR Curve
	#print("Generating PR curve....")
	##graph_prec_recall(ytest, ypred, fnaming)
	##print("Getting ROC curve")
	##getroc_curve(ytest, ypredproba, fnaming)
	print("Getting confusion matrix.....")
	cmatrix = confusionmatrix(ytest, ypred)
	if calcftimpo:
		ftimpo = None
		ftimpo = pred.feature_importance(traindf)
		print("Feature Importance on test data: ", ftimpo)
	else:
		ftimpo = None

	return modelperf, ftimpo, cmatrix, perf

# Train-validation performance
def result_summary(pred):
	results = pred.fit_summary()
	print("[*]Summary of models fitting: ", results)
	return

def test_stack(Xtest, ytest, predstack, testdf, traindf):
	ypred = predstack.predict(Xtest)
	perf = predstack.evaluate_predictions(y_true=ytest, y_pred=ypred, auxiliary_metrics= True)
	print("[*]Predictions: ", ypred)
	test_perf = predstack.leaderboard(testdf, silent=True)
	print("$$$$$$$$ RESULT STACKING $$$$$$$$\n", test_perf)
	ftimpo = None
	print("Performance evaluation: ", perf)
	#ftimpo = predstack.feature_importance(traindf)
	print("Feature Importance on test data: ", ftimpo)
	return test_perf, ftimpo, perf


# Performance evaluation on test data (part from experiment dataset)
def evaluate(Xtest, ytest, mpath= "agModels/5_True_imb"):
	pred = task.load(mpath) # /models/LightGBM"
	allmodels = pred.get_model_names()
	bestmodel = pred.get_model_best()
	print("All model names: ", allmodels,"\nBest: ",bestmodel)
	print("Ytest: ", ytest[0:10])
	ypred = pred.predict(Xtest)
	ypredproba = pred.predict_proba(Xtest)
	print("Ypred: ", ypred[0:10])
	print("Ypredprob: ", ypredproba[0:10])
	perf = pred.evaluate_predictions(y_true=ytest, y_pred=ypredproba, auxiliary_metrics= True)
	tn, fp, fn, tp = confusion_matrix(ytest, ypred).ravel()
	fpr = fp/(fp+tn)
	tpr = tp/(tp+fn)
	print("tn:%d fp:%d fn:%d tp:%d"%(tn, fp, fn, tp))
	print("FPR(%): ", fpr*100)
	print("TPR(%): ", tpr*100)
	avgprec = average_precision_score(ytest, ypredproba.iloc[:, 1])
	print("Autogluon evaluation: ", perf)
	print("Avg. precision score: ", avgprec)
	print("Len ytest/pred/predproba: ", ytest.size, ypred.size, ypredproba.size)

	return [tn, fp, fn, tp, fpr, tpr, perf, avgprec]


def zerodaytest(datadf, malinst, hostfts, modelpath, ff):
	print("Loading models for Malware: ", malinst, "HostFts?: ", hostfts)
	# split data to train/test
	featdf = datadf.iloc[:,:-1].copy() # drop target column
	labels = datadf.iloc[:,-1].copy()
	print("Zero day testing on: ", featdf.shape, labels)
	# fname = modelpath.split("/")[-1]
	# print("Model: ", modelpath, fname)
	print(featdf.size, labels.size)
	[tn, fp, fn, tp, fpr, tpr, perf, avgprec] = evaluate(featdf, labels, modelpath)
	ff.write("Zero day results: "+modelpath+"\n")
	ff.write("Confusion matrix: tn:%d fp:%d fn:%d tp:%d"%(tn, fp, fn, tp))
	ff.write("\n")
	ff.write("FPR(%): "+str(fpr*100)+"\n")
	ff.write("TPR(%): "+str(tpr*100)+"\n")
	ff.write(str(perf))
	ff.write("\nAverage Precision Score(%): "+str(avgprec*100))
	ff.write("\n==========================\n\n")
	return


# Note: Xvtraindf should exclude test features and target values
def main_1(datadf, targetcol, malinst, hostfts, topk):
	# split data to train/test
	featdf = datadf.iloc[:,:-1].copy() # drop target column
	labels = datadf.iloc[:,-1].copy()
	print("Train: 70% connections, Test: 30% connections")
	Xtrain, Xtest, ytrain, ytest = mll.split_df(featdf, labels)
	'''
	# Test prints
	print("Featdf(FULL DATA): ", featdf, featdf.shape)
	print("Labels: ", labels, labels.shape)
	print("Xtrain: ", Xtrain, Xtrain.shape)
	print("ytrain: ", ytrain, ytrain.shape)
	print("Xtest: ", Xtest, Xtest.shape)
	print("ytest: ", ytest, ytest.shape)
	print("Type after split: ",type(Xtrain), type(ytrain), type(Xtest), type(ytest))
	'''
	traindf = pd.concat([Xtrain, ytrain], axis=1)
	testdf = pd.concat([Xtest, ytest], axis=1)
	traindf.columns = [*traindf.columns[:-1], 'target']
	testdf.columns = [*testdf.columns[:-1], 'target']
	print("Train df (w/ target): ",traindf, traindf.shape)
	print("Train mal: ", traindf[traindf['target'] == 1].shape, "Train ben: ", traindf[traindf['target'] == 0].shape)
	print("Test df (w/ target): ",testdf, testdf.shape)
	print("Test mal: ", testdf[testdf['target'] == 1].shape, "Test ben: ", testdf[testdf['target'] == 0].shape)
	problem_type = infer_problem_type(labels)
	print("Autogluon detected problem type: ", problem_type)
	time.sleep(10)
	predictor = train1(traindf, targetcol, problem_type, malinst, hostfts, topk)
	# Stacking
	predstack = train_multilayerstacking(traindf, targetcol, problem_type, malinst, hostfts, topk)
	#result_summary(predstack)
	print("###################All Model Testing############################")
	# No ft impo --- quick test results
	res1, fimp1, cmatrix, perf_test = test1(Xtest, ytest, predictor, testdf, traindf, False)
	# Wth ft impo --- longer to return results 
	##res1, fimp1, cmatrix = test1(Xtest, ytest, predictor, testdf, traindf, fnaming)
	print("####################Stacking & Weighted Ensemble Testing###########################")
	res2, fimp2, perf_stest = test_stack(Xtest, ytest, predstack, testdf, traindf)

	return [res1, res2, fimp1, fimp2, cmatrix, perf_test, perf_stest]

'''
Script invoking SKLEARN Multi label techniques on provided dataset
'''
import os
import sys
import loaders as ll
import loaders_multilabel as mll
from scipy.sparse import csr_matrix

def main_ml(d, malfnames, benfnames, foldtotal, maltotal, malinst, topk, hostfts):
	# Read in 2 files:Map: malhash-int map, AVCLASS: malhash-multi labels
	proceed = ll.check_dependencies(d)
	multiclass = 2
	#[malfnames, benfnames, foldtotal, multiclass, maltotal, malinst, hostfts] = ll.get_list(d)
	if proceed:
		[avclassfile, reports] = ll.getfiles(d)
        # {shahash: avclass labels}
		classdt = ll.read_multilabels(avclassfile, reports)
        # {malwareindex: [labels, malware shahash]}
		sha_label_map = ll.sha_mapi(d, classdt)
		# GRAPH 1: Tag distribution per malware: Frequency/count of each tag in malware dataset
		# STATS: min tag per mal, max tag per mal, avg no. of tags per mal
		mll.label_stats(sha_label_map)
        # dt: {fpath: [0,1,0,1,1]} based on MultilabelBinarizer
		[labeldt, classorder, mlb] = ll.label_multiclass_multilabel(malfnames, benfnames, sha_label_map)
		print(classorder, len(classorder))
		#print(labeldt)
		#print(len(labeldt))
        # 3. feature extraction
		[featdf, xindex] = ll.extract_features(labeldt, multiclass, hostfts, top=topk, classorder=classorder)
		labels = None
		#print(featdf)
        # 4. classify
		##labelindex = -1
		labels = featdf.loc[:, list(classorder)].to_numpy()
		#labels = csr_matrix(labels_nd)
		Xdata = featdf.iloc[:,:xindex].to_numpy()
		# Test prints
		print("Features: \n", Xdata)
		print("Labels: \n", labels)
		print(type(Xdata), type(labels), "Per element type: ",type(Xdata[0]), type(labels[0]))
		print("Feature and label DF shape: ", Xdata.shape, labels.shape)

		'''
		print(featdf.columns[xindex-1])
		print(featdf.columns[xindex])
		return
		'''
		# Train-test split
		Xtrain, Xtest, ytrain, ytest = mll.split_df(Xdata, labels)
		print(type(Xtrain), type(Xtest), type(ytrain), type(ytest))
		print("Xtrain: ", Xtrain.shape, "ytrain: ",ytrain.shape)
		print("Xtest: ", Xtest.shape, "ytest: ", ytest.shape)
		print("Type after split: ",type(Xtrain), type(ytrain), type(Xtest), type(ytest))
		# Data normalization and standardization
		##Xtrainscaled, Xtestscaled = mll.normalize(Xtrain, Xtest)
		Xtrainscaled, Xtestscaled, ss = mll.standardize(Xtrain, Xtest)
		#graph_builder = mll.buildlabel_graph(Xtrainscaled, ytrain, list(classorder))
		#sys.exit()

		#print("Xtrainscaled:\n", Xtrainscaled)
		#print("Xtestscaled: \n", Xtestscaled)

		# Classifiers & Techniques
		# 1. Base: rf, knn
		print("Base random forest:: ")
		mll.base_rf(Xtrainscaled, ytrain, Xtestscaled, ytest, mlb)
		#mll.base_knn(Xtrainscaled, ytrain, Xtestscaled, ytest, mlb)

		# ------ Problem Transformation --------
		# 3. Binary Relevance: rf, knn, NB, LR
		brr1=[]
		brr2=[]
		print("BR::")
		# BRKNN: Classifier assigns labels assigned to >= half of the neighbors
		##mll.BRKNNA(Xtrainscaled, ytrain, Xtestscaled, ytest, mlb)
		brr1 = mll.BR(ss, Xtrainscaled, ytrain, Xtestscaled, ytest, mlb, classorder, topk, "rf")
		#brr2 = mll.BR(Xtrainscaled, ytrain, Xtestscaled, ytest, mlb, "knn")
		print("CC::")
		cc1=[]
		cc2=[]
		# 4. Classifier Chains: rf, knn
		cc1 = mll.ClassifierChain(ss, Xtrainscaled, ytrain, Xtestscaled, ytest, mlb, classorder, topk, "rf")
		#cc2 = mll.ClassifierChain(Xtrainscaled, ytrain, Xtestscaled, ytest, mlb, "knn")
		print("LP::")
		# 5. Labelpowerset: rf, knn
		lp1=[]
		lp2=[]
		lp1 = mll.labelpowerset(ss, Xtrainscaled, ytrain, Xtestscaled, ytest, mlb, classorder, topk, "rf")
		#lp2 = mll.labelpowerset(Xtrainscaled, ytrain, Xtestscaled, ytest, mlb,"knn")

		print("Label order: ", classorder, len(classorder))



	return [brr1,cc1,lp1]
